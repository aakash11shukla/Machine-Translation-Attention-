{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding, Dense, Lambda, LSTM, Bidirectional, Input, RepeatVector, Concatenate, Dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "    from keras.layers import CuDNNLSTM as LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100\n",
    "LATENT_DIM = 256\n",
    "VOCAB_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SPLIT = 0.2\n",
    "NUM_SAMPLES = 10000\n",
    "EPOCHS=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading dataset...')\n",
    "\n",
    "input_texts = []\n",
    "target_text_input = []\n",
    "target_text = []\n",
    "with open('D:/Downloads/dataset/spa-eng/spa.txt', encoding='utf8') as f:\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        count += 1\n",
    "        if count > NUM_SAMPLES:\n",
    "            break\n",
    "        \n",
    "        if '\\t' not in line:\n",
    "            continue\n",
    "        input_, translation = line.rstrip().split('\\t')\n",
    "        input_texts.append(input_)\n",
    "        target_text_input.append('<sos> ' + translation)\n",
    "        target_text.append(translation + ' <eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "encoded_tokenizer.fit_on_texts(input_texts)\n",
    "encoded_sequence = encoded_tokenizer.texts_to_sequences(input_texts)\n",
    "\n",
    "word2idx_inputs = encoded_tokenizer.word_index\n",
    "\n",
    "print('No of input words %d' % (len(word2idx_inputs)+1))\n",
    "\n",
    "max_len_input = max([len(s) for s in encoded_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters='')\n",
    "decoded_tokenizer.fit_on_texts(target_text + target_text_input)\n",
    "target_sequence = decoded_tokenizer.texts_to_sequences(target_text)\n",
    "target_input_sequence = decoded_tokenizer.texts_to_sequences(target_text_input)\n",
    "\n",
    "word2idx_target = decoded_tokenizer.word_index\n",
    "num_words_output = len(word2idx_target) + 1\n",
    "\n",
    "max_len_target = max(len(s) for s in target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequence = pad_sequences(encoded_sequence, maxlen=max_len_input)\n",
    "print('encoded inputs shape', encoded_sequence.shape)\n",
    "\n",
    "target_sequence = pad_sequences(target_sequence, maxlen=max_len_target, padding='post')\n",
    "target_input_sequence = pad_sequences(target_input_sequence, maxlen=max_len_target, padding='post')\n",
    "print('target_input_sequence shape', target_input_sequence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "with open('D:/Downloads/glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "    print('No of word vectors %d' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filling pre-trained...')\n",
    "num_words_input = min(VOCAB_SIZE, len(word2idx_inputs)+1)\n",
    "embedding_matrix = np.zeros((num_words_input, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < VOCAB_SIZE:\n",
    "        embedding_vector = word2idx_inputs.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embedding = Embedding(\n",
    "    num_words_input,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_len_input,\n",
    "    trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output_one_hot = np.zeros((len(input_texts), max_len_target, num_words_output), dtype='float32')\n",
    "\n",
    "for i, d in enumerate(target_sequence):\n",
    "    for j, word in enumerate(d):\n",
    "        decoder_output_one_hot[i, j, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "encoder_x = encoder_embedding(encoder_inputs_placeholder)\n",
    "encoder_lstm = Bidirectional(LSTM(LATENT_DIM, return_sequences=True))\n",
    "encoder_outputs = encoder_lstm(encoder_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_over_time(x):\n",
    "    assert(K.ndim(x) > 2)\n",
    "    e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "    s = K.sum(x, axis=1, keepdims=True)\n",
    "    return e/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_repeat_layer = RepeatVector(max_len_input)\n",
    "attention_concat_layer = Concatenate(axis=-1)\n",
    "attention_dense_layer1 = Dense(10, activation='tanh')\n",
    "attention_dense_layer2 = Dense(1, activation=softmax_over_time)\n",
    "attention_dot_layer = Dot(axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(h, st_1):\n",
    "    \n",
    "    st_1 = attention_repeat_layer(st_1)\n",
    "    x = attention_concat_layer([h, st_1])\n",
    "    x = attention_dense_layer1(x)\n",
    "    alphas = attention_dense_layer2(x)\n",
    "    context = attention_dot_layer([alphas, h])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM, trainable=True)\n",
    "decoder_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_state=True)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_s = Input(shape=(LATENT_DIM,), name='s0')\n",
    "initial_c = Input(shape=(LATENT_DIM,), name='c0')\n",
    "context_last_word_concat_layer = Concatenate(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "s = initial_s\n",
    "c = initial_c\n",
    "for t in range(max_len_target):\n",
    "    context = one_step_attention(encoder_outputs, s)\n",
    "    selector = Lambda(lambda x: x[:, t:t+1])\n",
    "    xt = selector(decoder_x)\n",
    "    decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "    o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
    "    decoder_outputs = decoder_dense(o)\n",
    "    outputs.append(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_and_transpose(x):\n",
    "    x = K.stack(x)\n",
    "    x = K.permute_dimensions(x, pattern=(1, 0, 2))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    inputs =[\n",
    "        encoder_inputs_placeholder,\n",
    "        decoder_inputs_placeholder,\n",
    "        initial_s,\n",
    "        initial_c\n",
    "    ],\n",
    "    outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.zeros((NUM_SAMPLES, LATENT_DIM))\n",
    "r = model.fit(\n",
    "    [encoded_sequence, target_input_sequence, z, z],\n",
    "    decoder_output_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='valid_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='valid_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output_as_input = Input(shape=(max_len_input, LATENT_DIM*2,))\n",
    "decoder_input_single = Input(shape=(1,))\n",
    "decoder_input_x = decoder_embedding(decoder_input_single)\n",
    "context = one_step_attention(encoder_output_as_input, initial_s)\n",
    "decoder_lstm_input = context_last_word_concat_layer([context, decoder_input_x])\n",
    "\n",
    "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "decoder_outputs = decoder_dense(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(\n",
    "    inputs=[encoder_output_as_input,\n",
    "           decoder_input_single,\n",
    "           initial_s,\n",
    "           initial_c],\n",
    "    outputs=[decoder_outputs, s, c]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_input = {v:k for k,v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k,v in word2idx_target.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    \n",
    "    enc_out = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0,0] = word2idx_target['<sos>']\n",
    "    eos = word2idx_target['<eos>']\n",
    "    \n",
    "    s = np.zeros((1, LATENT_DIM))\n",
    "    c = np.zeros((1, LATENT_DIM))\n",
    "    \n",
    "    output_sentence = []\n",
    "    \n",
    "    for _ in range(max_len_target):\n",
    "        o, s, c = decoder_model.predict([enc_out, target_seq, s, c])\n",
    "        \n",
    "        idx = np.argmax(o.flatten())\n",
    "        \n",
    "        if idx == eos:\n",
    "            break\n",
    "        \n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_target[idx]\n",
    "            output_sentence.append(word)\n",
    "            \n",
    "        target_seq[0, 0] = idx\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    i = np.random.choice(len(input_texts))\n",
    "    input_seq = encoded_sequence[i:i+1]\n",
    "    translation = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('input_sequence', input_texts[i])\n",
    "    print('Predicted translation', translation)\n",
    "    print('Actual translation', target_text[i])\n",
    "    \n",
    "    ans = input('Continue? [Y/n]')\n",
    "    \n",
    "    if ans and ans.lower().startswith('n'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
